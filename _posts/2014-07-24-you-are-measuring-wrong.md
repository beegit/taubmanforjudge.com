---

layout: post
title:  The way you're measuring content is all wrong
date:   2014-07-24 12:00:00
teaser: Measuring content has become big business. There are marvelous and impressive tools for measuring what happens to your content the moment you share it with the world, showing you likes, engagement, shares, retweets and so on.  But how do you analyze and measure what went into creating that content? 
image: http://a.abcnews.com/images/Politics/GTY_supreme_court_cases_jef_131003_16x9_992.jpg
comments: true
category: whaaaatttt?
shortUrl: 

---

Measuring content has become big business. There are marvelous and impressive tools for measuring what happens to your content the moment you share it with the world, showing you likes, engagement, shares, retweets and so on. 

But how do you analyze and measure what went into creating that content? 

Traditionally, it hasn't been possible. The whole [Beegit](https://beegit.com) team must be tired of hearing me tell the story of my old boss who regularly asked me how much our freelance writers cost the company. 

The only information I could give him was how much we paid freelance writers - but that didn't answer his question. He wanted details on how much one writer cost him versus another once the content went into production. He wanted to know which freelance writer required the least amount of rewrites and which of our in-house editors cleaned up copy the fastest. 

When it comes to content creation, the only real success metric most teams have is a deadline. They hit it or they don't. Everything that happens up to that deadline is considered impossible to review and repeat. 

Fixing this requires more data. To create something measurable, many content teams are now using manual time tracking applications like Harvest or Tsheets, but that's only a fraction of the solution. Writers can tell you that time spent, without the context of what work was done, is not directly correlated with quality.

So when good content is created, what *is* measurable? 

In the right production process, you can get real insights when you have historical data about who wrote a piece, who edited it, and when that happened in the production process. It requires information on things like who wrote a headline and who tweaked it. But these measurables require the right word processor. 

If you look at a Word document, it doesn't tell you that story. If you look at Google Docs, you can start to piece some details together if you're patient enough to scroll through the revision history. 

At [Beegit](https://beegit.com), we're providing this information through checked in milestones during the creation of a version history. That means that anyone working on content can describe what they did in a version summary so others can get an idea of project progress.

![Versioning](/assets/versioning.jpg)

This, in addition to change tracking that shows who added and deleted every word and when they did it, produces real data. It gives you the ability to see how much editing a writer requires and who is putting together the words that end up going to production. That's the information you can use to measure who makes your content better.

That data gives you powerful advice on how you can improve your content development. When you have that, you can create another piece of content with an output that's worth measuring. 

Our goal at [Beegit](https://beegit.com/) is to make a centralized hub for  all of your content while everything from production analytics to universal export is handled for you. We're in our closed beta now, and we'd love to have you [join us](https://beegit.com) for our open beta, coming later this year. 
